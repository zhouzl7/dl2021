# hw2: report

**姓名：周泽龙**
**学号：2020213990**
**课程：深度学习**
**日期：2021年5月16日**

------

[TOC]



<div STYLE="page-break-after: always;"></div>

## Task A: standard RNN [30pts]

In task A, I construct a standard RNN (including LSTM, GRU). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* RNN type
  * LSTM & GRU
* Different number of layers 
  * 1 & 2 & 4 & 8 & 16
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|     Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|   LSTM1_clip   |           |           |          |          |     20722478     |
| LSTM1_original |           |           |          |          |     20722478     |
|   LSTM2_clip   |           |           |          |          |     21444878     |
|   LSTM4_clip   |           |           |          |          |     22889678     |
|   LSTM8_clip   |           |           |          |          |     25779278     |
|   GRU1_clip    |           |           |          |          |     20541878     |

> **ps:** LSTM2 means: nlayers = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. LSTM1_Adam

Use Visdom to visualize training and validation curves. The training and validation curves are as follows:





## Task B: standard Transformer [30pts]

In task B, I construct a standard Transformer (Attention is All You Need). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* Different number of layers 
  * 2 & 4
* Different number of heads
  * 2 & 4
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|      Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :-------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|  Trans22_clip   |           |           |          |          |     21086078     |
| Trans22_orginal |           |           |          |          |     21086078     |
|  Trans42_clip   |           |           |          |          |     22172078     |
|  Trans24_clip   |           |           |          |          |     21086078     |

> **ps:** Trans42 means: nlayers = 4 and nhead = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. Trans22_Adam

Use Visdom to visualize training and validation curves. The training and validation curves are as follows:





## Other Tasks

#### 1. Data Preparation [10pts]



#### 2. Technical Details [10pts]



#### 3. Attention Visualization [10pts]



#### 4. Extra Techniques [10pts]

## references

[1] 

