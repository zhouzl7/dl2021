# hw2: report

**姓名：周泽龙**
**学号：2020213990**
**课程：深度学习**
**日期：2021年5月16日**

------

[TOC]



<div STYLE="page-break-after: always;"></div>

## Task A: standard RNN [30pts]

In task A, I construct a standard RNN (including LSTM, GRU). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* RNN type
  * LSTM & GRU
* Different number of layers 
  * 1 & 2 & 4 & 8 & 16
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|     Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|   LSTM1_clip   |   45.49   |  127.21   |  120.94  |    43    |     20722478     |
| LSTM1_original |           |           |          |          |     20722478     |
|   LSTM2_clip   |   56.27   |  123.40   |  116.42  |    51    |     21444878     |
|   LSTM4_clip   |   68.45   |  138.52   | 130.30[  |    82    |     22889678     |
|   LSTM8_clip   |  1021.24  |  985.30   |  965.01  |   131    |     25779278     |
|   GRU1_clip    |   45.06   |  125.96   |  119.69  |    85    |     20541878     |

> **ps:** LSTM2 means: nlayers = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. LSTM1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.80** loss and **120.94** ppl on the **test** set. The training and validation curves are as follows:

![LSTM1_loss](hw3_report_2020213990.assets/LSTM1_loss.svg)

##### 2.2. LSTM1_original

Use Visdom to visualize training and validation curves. This model achieves **100** loss and **100** ppl on the **test** set. The training and validation curves are as follows:



##### 2.3. LSTM2_clip

Use Visdom to visualize training and validation curves. This model achieves **4.76** loss and **116.42** ppl on the **test** set. The training and validation curves are as follows:

![LSTM2_loss](hw3_report_2020213990.assets/LSTM2_loss.svg)

##### 2.4. LSTM4_clip

Use Visdom to visualize training and validation curves. This model achieves **4.87** loss and **130.30** ppl on the **test** set. The training and validation curves are as follows:

![LSTM4_loss](hw3_report_2020213990.assets/LSTM4_loss.svg)

##### 2.5. LSTM8_clip

Use Visdom to visualize training and validation curves. This model achieves **6.87** loss and **965.01** ppl on the **test** set. The training and validation curves are as follows:

![LSTM8_loss](hw3_report_2020213990.assets/LSTM8_loss.svg)

##### 2.6. GRU1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.78** loss and **119.69** ppl on the **test** set. The training and validation curves are as follows:

![GRU1_loss](hw3_report_2020213990.assets/GRU1_loss.svg)



## Task B: standard Transformer [30pts]

In task B, I construct a standard Transformer (Attention is All You Need). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* Different number of layers 
  * 2 & 4
* Different number of heads
  * 2 & 4
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|      Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :-------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|  Trans11_clip   |           |           |          |          |     20543078     |
|  Trans22_clip   |           |           |          |          |     21086078     |
| Trans22_orginal |           |           |          |          |     21086078     |
|  Trans42_clip   |           |           |          |          |     22172078     |
|  Trans24_clip   |           |           |          |          |     21086078     |

> **ps:** Trans42 means: nlayers = 4 and nhead = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. Trans22_Adam

Use Visdom to visualize training and validation curves. This model achieves **100** loss and **100** ppl on the **test** set. The training and validation curves are as follows:





## Other Tasks

#### 1. Data Preparation [10pts]



#### 2. Technical Details [10pts]



#### 3. Attention Visualization [10pts]



#### 4. Extra Techniques [10pts]

## references

[1] 

