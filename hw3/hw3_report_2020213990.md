# hw2: report

**姓名：周泽龙**
**学号：2020213990**
**课程：深度学习**
**日期：2021年5月16日**

------

[TOC]



<div STYLE="page-break-after: always;"></div>

## Task A: standard RNN [30pts]

In task A, I construct a standard RNN (including LSTM, GRU). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* RNN type
  * LSTM & GRU
* Different number of layers 
  * 1 & 2 & 4 & 8 & 16
* Learning rate strategy
  * Adam
  * SGD

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|   Model    | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :--------: | :-------: | :-------: | :------: | :------: | :--------------: |
| LSTM1_Adam |           |           |          |          |     20722478     |
| LSTM1_SGD  |           |           |          |          |     20722478     |
| LSTM2_Adam |           |           |          |          |     21444878     |
| LSTM4_Adam |           |           |          |          |     22889678     |
| LSTM8_Adam |           |           |          |          |     25779278     |
| GRU1_Adam  |           |           |          |          |     20541878     |

> **ps:** LSTM2 means: nlayers = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. LSTM1_Adam

Use Visdom to visualize training and validation curves. The training and validation curves are as follows:





## Task B: standard Transformer [30pts]

In task B, I construct a standard Transformer (Attention is All You Need). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* Different number of layers 
  * 2 & 4
* Different number of heads
  * 2 & 4
* Learning rate strategy
  * Adam
  * SGD

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|    Model     | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :----------: | :-------: | :-------: | :------: | :------: | :--------------: |
| Trans22_Adam |           |           |          |          |     21086078     |
| Trans22_SGD  |           |           |          |          |     21086078     |
| Trans42_Adam |           |           |          |          |     22172078     |
| Trans24_Adam |           |           |          |          |     21086078     |

> **ps:** Trans42 means: nlayers = 4 and nhead = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. Trans22_Adam

Use Visdom to visualize training and validation curves. The training and validation curves are as follows:





## Other Tasks

#### 1. Data Preparation [10pts]



#### 2. Technical Details [10pts]



#### 3. Attention Visualization [10pts]



#### 4. Extra Techniques [10pts]

## references

[1] 

