# hw2: report

**姓名：周泽龙**
**学号：2020213990**
**课程：深度学习**
**日期：2021年5月16日**

------

[TOC]



<div STYLE="page-break-after: always;"></div>

## Task A: standard RNN [30pts]

In task A, I construct a standard RNN (including LSTM, GRU). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* RNN type
  * LSTM & GRU
* Different number of layers 
  * 1 & 2 & 4 & 8 & 16
* use gradient clip or not

```python
# Hyperparameter setting
ninput = 300
nhid = 300
lr = 0.001 if use Adam else 5
clip = 0.1
epochs = 100
train_batch_size = 20
eval_batch_size = 10
bptt = 100
dropout = 0.5
nlayers and nhead (Transformer) are control variables
```

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

| No.  |     Model      | Train PPL | Valid PPL  |  Test PPL  | ms/batch | Trainable params |
| :--: | :------------: | :-------: | :--------: | :--------: | :------: | :--------------: |
|  1   |   LSTM1_clip   |   48.28   |   125.68   |   119.51   |    43    |     20722478     |
|  2   | LSTM1_original |  943.23   |   662.08   |   622.36   |    53    |     20722478     |
|  3   | **LSTM2_clip** | **56.27** | **123.40** | **116.42** |  **51**  |   **21444878**   |
|  4   |   LSTM4_clip   |   68.45   |   138.52   |   130.30   |    82    |     22889678     |
|  5   |   LSTM8_clip   |  1021.24  |   985.30   |   965.01   |   131    |     25779278     |
|  6   |   GRU1_clip    |   45.06   |   125.96   |   119.69   |    85    |     20541878     |

> **ps1:** LSTM2 means: `nlayers` = 2
>
> **ps2:** clip means: use `gradient clip` and `Adam` optimizer; original means: only use `SGD` optimizer.

**Result analysis:**

* Compare 1 and 6, The effects of LSTM1 and GRU1 are similar, and both are around 119 `ppl`. However, even though LSTM1 has more trainable parameters than GRU1, LSTM1's efficiency is twice that of GRU1 (43 vs 85). The reason does not rule out that when GRU1 is training, the same GPU is occupied by other processes.
* Compare 1 and 2, `gradient clipping` can significantly improve LSTM.
* Compare 1, 3, 4 and 5, `nlayers` has a non-linear relationship with `ppl`, when `nlayers` is 2, `ppl` is the best, which is **116.42**.

#### 2. Training and validation curves.

##### 2.1. LSTM1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.78** loss and **119.51** ppl on the **test** set. The training and validation curves are as follows:

![LSTM1_loss](hw3_report_2020213990.assets/LSTM1_loss.svg)

##### 2.2. LSTM1_original

Use Visdom to visualize training and validation curves. This model achieves **6.43** loss and **622.36** ppl on the **test** set. The training and validation curves are as follows:

![LSTM1_original_loss](hw3_report_2020213990.assets/LSTM1_original_loss.svg)



##### 2.3. LSTM2_clip

Use Visdom to visualize training and validation curves. This model achieves **4.76** loss and **116.42** ppl on the **test** set. The training and validation curves are as follows:

![LSTM2_loss](hw3_report_2020213990.assets/LSTM2_loss.svg)

##### 2.4. LSTM4_clip

Use Visdom to visualize training and validation curves. This model achieves **4.87** loss and **130.30** ppl on the **test** set. The training and validation curves are as follows:

![LSTM4_loss](hw3_report_2020213990.assets/LSTM4_loss.svg)

##### 2.5. LSTM8_clip

Use Visdom to visualize training and validation curves. This model achieves **6.87** loss and **965.01** ppl on the **test** set. The training and validation curves are as follows:

![LSTM8_loss](hw3_report_2020213990.assets/LSTM8_loss.svg)

##### 2.6. GRU1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.78** loss and **119.69** ppl on the **test** set. The training and validation curves are as follows:

![GRU1_loss](hw3_report_2020213990.assets/GRU1_loss.svg)



## Task B: standard Transformer [30pts]

In task B, I construct a standard Transformer (Attention is All You Need). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* Different number of layers 
  * 2 & 4
* Different number of heads
  * 2 & 4
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

| No.  |      Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :--: | :-------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|  1   |  Trans11_clip   |  178.58   |  255.90   |  232.54  |    39    |     20543078     |
|  2   |  Trans22_clip   |  156.45   |  201.86   |  184.38  |    48    |     21086078     |
|  3   | Trans22_orginal |   61.16   |  328.09   |  303.34  |    49    |     21086078     |
|  4   |  Trans42_clip   |  147.38   |  178.75   |  163.98  |    61    |     22172078     |
|  5   |  Trans24_clip   |  151.26   |  195.54   |  178.75  |    49    |     21086078     |

> **ps:** Trans42 means: nlayers = 4 and nhead = 2
>
> **ps2:** clip means: use `gradient clip` and `SGD` optimizer; original means: only use `Adam` optimizer.

**Result analysis:**

* Compare 1, 2, 4 and 5, appropriately increasing `nlayers` and `nhead` can effectively increase ppl. In the limited experimental results, 4 `nlayers` and 2 `nhead` work best. Besides, `nlayers` and efficiency (ms/batch) are positively correlated.
* Compare 2 and 3, `gradient clipping` can significantly improve `Transformer`.

#### 2. Training and validation curves.

##### 2.1. Trans11_clip

Use Visdom to visualize training and validation curves. This model achieves **5.45** loss and **232.54** ppl on the **test** set. The training and validation curves are as follows:

![Transformer11_loss](hw3_report_2020213990.assets/Transformer11_loss.svg)

##### 2.2. Trans22_clip

Use Visdom to visualize training and validation curves. This model achieves **5.22** loss and **184.38** ppl on the **test** set. The training and validation curves are as follows:

![Transformer22_loss](hw3_report_2020213990.assets/Transformer22_loss.svg)

##### 2.3. Trans22_original

Use Visdom to visualize training and validation curves. This model achieves **5.71** loss and **303.34** ppl on the **test** set. The training and validation curves are as follows:

![Transformer22_original_loss](hw3_report_2020213990.assets/Transformer22_original_loss.svg)

##### 2.4. Trans42_clip

Use Visdom to visualize training and validation curves. This model achieves **5.10** loss and **163.98** ppl on the **test** set. The training and validation curves are as follows:

![Transformer42_loss](hw3_report_2020213990.assets/Transformer42_loss.svg)

##### 2.5. Trans24_clip

Use Visdom to visualize training and validation curves. This model achieves **5.19** loss and **178.75** ppl on the **test** set. The training and validation curves are as follows:

![Transformer24_loss](hw3_report_2020213990.assets/Transformer24_loss.svg)

## Other Tasks

#### 1. Data Preparation [10pts]



#### 2. Technical Details [10pts]



#### 3. Attention Visualization [10pts]



#### 4. Extra Techniques [10pts]



## references

[1] 

