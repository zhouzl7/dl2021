# hw2: report

**姓名：周泽龙**
**学号：2020213990**
**课程：深度学习**
**日期：2021年5月16日**

------

[TOC]



<div STYLE="page-break-after: always;"></div>

## Task A: standard RNN [30pts]

In task A, I construct a standard RNN (including LSTM, GRU). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* RNN type
  * LSTM & GRU
* Different number of layers 
  * 1 & 2 & 4 & 8 & 16
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|     Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|   LSTM1_clip   |   48.28   |  125.68   |  119.51  |    43    |     20722478     |
| LSTM1_original |  943.23   |  662.08   |  622.36  |    53    |     20722478     |
|   LSTM2_clip   |   56.27   |  123.40   |  116.42  |    51    |     21444878     |
|   LSTM4_clip   |   68.45   |  138.52   |  130.30  |    82    |     22889678     |
|   LSTM8_clip   |  1021.24  |  985.30   |  965.01  |   131    |     25779278     |
|   GRU1_clip    |   45.06   |  125.96   |  119.69  |    85    |     20541878     |

> **ps:** LSTM2 means: nlayers = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. LSTM1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.78** loss and **119.51** ppl on the **test** set. The training and validation curves are as follows:

![LSTM1_loss](hw3_report_2020213990.assets/LSTM1_loss.svg)

##### 2.2. LSTM1_original

Use Visdom to visualize training and validation curves. This model achieves **6.43** loss and **622.36** ppl on the **test** set. The training and validation curves are as follows:

![LSTM1_original_loss](hw3_report_2020213990.assets/LSTM1_original_loss.svg)



##### 2.3. LSTM2_clip

Use Visdom to visualize training and validation curves. This model achieves **4.76** loss and **116.42** ppl on the **test** set. The training and validation curves are as follows:

![LSTM2_loss](hw3_report_2020213990.assets/LSTM2_loss.svg)

##### 2.4. LSTM4_clip

Use Visdom to visualize training and validation curves. This model achieves **4.87** loss and **130.30** ppl on the **test** set. The training and validation curves are as follows:

![LSTM4_loss](hw3_report_2020213990.assets/LSTM4_loss.svg)

##### 2.5. LSTM8_clip

Use Visdom to visualize training and validation curves. This model achieves **6.87** loss and **965.01** ppl on the **test** set. The training and validation curves are as follows:

![LSTM8_loss](hw3_report_2020213990.assets/LSTM8_loss.svg)

##### 2.6. GRU1_clip

Use Visdom to visualize training and validation curves. This model achieves **4.78** loss and **119.69** ppl on the **test** set. The training and validation curves are as follows:

![GRU1_loss](hw3_report_2020213990.assets/GRU1_loss.svg)



## Task B: standard Transformer [30pts]

In task B, I construct a standard Transformer (Attention is All You Need). Use Nvidia RTX 1080 to accelerate my experiment, the following experiments will focus on these aspects:

* Different number of layers 
  * 2 & 4
* Different number of heads
  * 2 & 4
* use gradient clip or not

#### 1. PPL & Time

Sorry for not having enough time to go through all possible situations. The summary results are shown in the table below.

|      Model      | Train PPL | Valid PPL | Test PPL | ms/batch | Trainable params |
| :-------------: | :-------: | :-------: | :------: | :------: | :--------------: |
|  Trans11_clip   |  178.58   |  255.90   |  232.54  |    39    |     20543078     |
|  Trans22_clip   |  156.45   |  201.86   |  184.38  |    48    |     21086078     |
| Trans22_orginal |   61.16   |  328.09   |  303.34  |    49    |     21086078     |
|  Trans42_clip   |  147.38   |  178.75   |  163.98  |    61    |     22172078     |
|  Trans24_clip   |  151.26   |  195.54   |  178.75  |    49    |     21086078     |

> **ps:** Trans42 means: nlayers = 4 and nhead = 2

**Result analysis:**

* Compared with the model (DilateNet18[0,0,0,0]) that does not use `DilateBlock`, the accuracy of the DilateNet18[0,0,1,1] is improved (training accuracy: 90.60% to 94.50%, test accuracy: 86.01% to 87.76%).
* Comparing DilateNet18[0,0,1,1] and DilateNet34[0,0,1,1], the depth of the model does not necessarily have a positive effect on the accuracy
* Different replacements have an impact on accuracy. In the limited experimental results, [0,0,1,1] is the best.



#### 2. Training and validation curves.

##### 2.1. Trans11_clip

Use Visdom to visualize training and validation curves. This model achieves **5.45** loss and **232.54** ppl on the **test** set. The training and validation curves are as follows:

![Transformer11_loss](hw3_report_2020213990.assets/Transformer11_loss.svg)

##### 2.2. Trans22_clip

Use Visdom to visualize training and validation curves. This model achieves **5.22** loss and **184.38** ppl on the **test** set. The training and validation curves are as follows:

![Transformer22_loss](hw3_report_2020213990.assets/Transformer22_loss.svg)

##### 2.3. Trans22_original

Use Visdom to visualize training and validation curves. This model achieves **5.71** loss and **303.34** ppl on the **test** set. The training and validation curves are as follows:

![Transformer22_original_loss](hw3_report_2020213990.assets/Transformer22_original_loss.svg)

##### 2.4. Trans42_clip

Use Visdom to visualize training and validation curves. This model achieves **5.10** loss and **163.98** ppl on the **test** set. The training and validation curves are as follows:

![Transformer42_loss](hw3_report_2020213990.assets/Transformer42_loss.svg)

##### 2.5. Trans24_clip

Use Visdom to visualize training and validation curves. This model achieves **5.19** loss and **178.75** ppl on the **test** set. The training and validation curves are as follows:

![Transformer24_loss](hw3_report_2020213990.assets/Transformer24_loss.svg)

## Other Tasks

#### 1. Data Preparation [10pts]



#### 2. Technical Details [10pts]



#### 3. Attention Visualization [10pts]



#### 4. Extra Techniques [10pts]

## references

[1] 

